# [MSDS 686] Week 3 Eye Tracking Update (Sep 15, 2018)

<img src="https://github.com/rcdilorenzo/msds-686-eye-tracking/blob/master/visualizations/face-landmark-sample.png?raw=true">


Although I spent the first part of this week out at the NC beach with some extended family, we ended up with a quick return due to Hurricane Florence. During this time, however, I was able to do a bit of reading and remotely do some school on [my custom computer](https://camo.githubusercontent.com/ccc200b8fc8d5d9b6adc2349e8a2acabcbbe240b/68747470733a2f2f63646e2d696d616765732d312e6d656469756d2e636f6d2f6d61782f3630302f312a49635251317a4d5741322d4e514b79517a4d386e6a412e706e67) using some software I wrote ([Haskell server](https://github.com/rcdilorenzo/tunneler-server) / [iOS + watchOS client](https://github.com/rcdilorenzo/tunneler-client)).

## 3D Facial Landmarks, Open Images v4, Yolo v3, and 2TB RAID upgrade

**Face Alignment**

With the disappointing face and eye detection present by default in the GazeCapture data set (see "Notes" from [last week's post](week-02-dated-09-08-2018.md)), I began with some reading about object detection and what other researchers have done for face and eye detection. I stumbled upon a facial landmarking library called [face-alignment](https://github.com/1adrianb/face-alignment) and read the paper to gain some insight into a potentially alternative approach. At this point, I realized that it might be possible to use this project to predict eye gaze strictly from the 3D landmark points to the relative gaze position to the camera without any CNNs (convolutional neural networks). The library certainly did not talk much about speed, but this approach might give a much better accuracy than the iTracker model that the GazeCapture researchers originally developed.

To this end, I wrote some Python code ([landmark.py](https://github.com/rcdilorenzo/msds-686-eye-tracking/blob/cecfd164ef9aa7211f7c988ff0d0b1bf81865b41/02-facial-landmarks/landmark.py)) to perform the facial landmark detection on the ~ 2.4 million images of GazeCapture and save every 100K `numpy` matrices to disk. Unfortunately, the first batch took nearly four hours. Over the course of the next five days, I continued to monitor, tweak, parallelize, and save the results to the repository. I even overclocked my i7700K by 0.4 GHz (totaling 4.6 GHz) to take full advantage of the water cooling and parallelization.

**Yolo v3 & Open Images v4**

Simultaneously, I took the time to contemplate working with a pre-trained model. I considered ResNet since I have had some success with it in a brief experiment several months ago. However, I knew from experience that [Yolo](https://pjreddie.com/darknet/yolo/) is pretty much still the best balance of speed and accuracy. With the end goal of live eye gazing in mind, I directed my attention to learning about the newly released third version. Because the GazeCapture data set was simply using the real-time iOS object detection for face and eyes, I did not have a labeled data set to detect eyes.

Having heard of the fourth release of [Open Images](https://storage.googleapis.com/openimages/web/2018-04-30-announcing-v4-and-challenge.html) several months ago, I circled back to see what might be reasonably obtained. Although I knew the entire data set was 18TB, I found that the 1.9 million subset [available from figure eight](https://www.figure-eight.com/dataset/open-images-annotated-with-bounding-boxes/) totalled ~ 560 GB and providentially included "Human eye" as one of the 600 class labels.

**2TB 7200RPM RAID Upgrade**

Because I run an encrypted LUKS partition (ext4) in Linux on a 1TB SSD, I could not store everything on the drive. Unfortunately, my 5400RPM backup drive was going to be a constant hindrance to keeping the data analysis zippy for experimentation. With future data storage in mind, I took this opportunity to grab two WD Black 1TB 7200 hard drives for a fantastic price of $59.99 each (kudos to price-matching Gary from BestBuy). After installing them and setting up RAID 0 (including some scary moments, lots of patience, and a bit of back pain), I was able to transfer the partial downloads from the Open Images v4 (OIv4) subset overnight to this drive.

<img alt="hard drives and upgraded power supply (for some other hardware coming)" src="https://user-images.githubusercontent.com/634167/45592514-6f860d80-b93d-11e8-8adc-90cf328d4313.jpg" width="60%" />
<br><br>

**Open Images v4 & keras-yolo3**

With the images from the OIv4 and the many metadata files downloaded, I needed to not only understand how all of the data structure fits together (train, validation, and test) but also convert the format to run the stock build of Yolo v3 against that data set. Because this class is focused on designing and implementing neural networks using `keras`, I looked for a project that I could use as the base model for my own `keras` code. Unfortunately, the projects I did find had their own wrapping code for the Yolo v3 layers they reproduced with `keras`. To therefore minimize the amount of experimental variables at play, I opted to just focus on converting the metadata format of the OIv4 images that included regions labeled as "Human eye" to the space and comma delimited format Yolo v3 expects.

The project I found that ports Yolo v3 to `keras` is [keras-yolo3](https://github.com/qqwweee/keras-yolo3). As mentioned, I plan to dig through this code in the coming week and just use the layers, but for the present I needed to focus on getting something to work. All of this data processing is included in the  [`openimage-eye-detection-yolo-v3` notebook](https://github.com/rcdilorenzo/msds-686-eye-tracking/blob/cecfd164ef9aa7211f7c988ff0d0b1bf81865b41/01-yolov3/openimage-eye-detection-yolo-v3.ipynb) that includes a brief EDA to make sure I was assuming the correct coordinate system. 

**Conclusion & Next Week**

Unfortunately, I simply ran out of time this week to get a functional neural network model because of the two approaches I have been simultaneously pursuing. Every time I thought I had things "almost" there, another transformation was necessary or I accidentally overwrote the facial landmarks arrays after the final iteration (and thereby losing ~ 3+ hours of computational work). However, I do have everything setup for the coming week as soon as I resolve some technical issues with `keras-yolo3`.

I expect both the facial landmark approach as well as the eye detection training with Yolo v3 to be the primary focus of this upcoming week. Visualizing and scrutinizing with TensorBoard will be a must. If I get a decent model from the eye detection approach, I plan to use that as the basis for next stage--gaze prediction. For that, I'll have to turn back to the GazeCapture data and somehow relate the detection region of one or more eyes to a particular gaze. As previously meneioned, though, I will try to punt on as many of these edge cases as possible in order have my first "make it run" stage.


## Resources

- Face Alignment ([homepage](https://adrianbulat.com/face-alignment) / [GitHub](https://github.com/1adrianb/face-alignment) / [paper](https://arxiv.org/pdf/1703.07332.pdf))
- Yolo v3 ([Website](https://pjreddie.com/darknet/yolo/) / [Paper](https://pjreddie.com/media/files/papers/YOLOv3.pdf))
- [What's new in YOLO v3?](https://towardsdatascience.com/yolo-v3-object-detection-53fb7d3bfe6b)
- Open Images v4 ([Website](https://storage.googleapis.com/openimages/web/index.html) / [Subset](https://www.figure-eight.com/dataset/open-images-annotated-with-bounding-boxes/))
- [keras-yolo3](https://github.com/qqwweee/keras-yolo3)


## References

Bulat, A., & Tzimiropoulos, G. (2017). How far are we from solving the 2D & 3D face alignment problem? (and a Dataset of 230,000 3D Facial Landmarks). *2017 IEEE International Conference on Computer Vision (ICCV)*. doi:10.1109/iccv.2017.116
