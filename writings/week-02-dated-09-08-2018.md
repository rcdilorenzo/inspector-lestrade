# [MSDS 686] Week 2 Eye Tracking Update (Sep 1, 2018)

<img alt="blurred faces with boxes" src="https://raw.githubusercontent.com/rcdilorenzo/msds-686-eye-tracking/master/visualizations/faces-blurred-grayscale.png" width="50%">

## Exploratory Data Analysis (EDA)

(Link: [https://github.com/rcdilorenzo/msds-686-eye-tracking/blob/master/eye-tracking-eda.ipynb](https://github.com/rcdilorenzo/msds-686-eye-tracking/blob/master/eye-tracking-eda.ipynb))

Using the [GazeCapture data](http://gazecapture.csail.mit.edu) collected by seven researchers across three universities, I have been primarily focused this week on the exploratory data analysis of the data set while concurrently exploring Deep Learning architectures (see resources below). This [data set](http://gazecapture.csail.mit.edu) includes +1,450 participants that comprise a database of camera snapshots of each participant gazing at a dot generated on the screen of their iOS device. Through the EDA, I was able to discover not only the 2.4 million frames but also that the iOS devices range from an iPhone 4s to an iPad Pro. In fact, I wrote a brief Medium.com article ([Visualization Quick Tip: Relative Heatmaps](https://medium.com/@rcdilorenzo/visualization-quick-tip-relative-heatmaps-86a52a0c5a0c)) that walks through the point visualization in the "Dot Info" of the notebook (linked above).

Likewise, I was able to discover the relative camera projection of the selected dot points. This will more than likely be the target of my final neural network. The goal is to eventually be able to directly take a webcam image and see where the person is looking on the mobile screen. However, after seeing more than several visuals of the Apple-provided detection regions of people's face and eyes, I may resort to using a non-realtime algorithm to detect the faces and thereby feed my neural network gaze prediction. 

Next week, I am going to have to do much more reading to understand object detection and try it out on some sample dataset. I expect I may have less visuals to show but hopefully a much better understanding of how to model things going forward.

## Experiments

- To facilitate my learning alongside reading Chollet (see resources), I started experimenting with the MNIST data set while simultaneously visualizing the different model iterations with TensorBoard (see resources). 
- I also began reading on object detection with CNNs both in Chollet and various articles:
    - [Real-time Object Detection with YOLO, YOLOv2 and now YOLOv3](https://medium.com/@jonathan_hui/real-time-object-detection-with-yolo-yolov2-28b1b93e2088)
    - [Object detection with neural networks—a simple tutorial using keras](https://towardsdatascience.com/object-detection-with-neural-networks-a4e2c46b4491)
    - [Fashion-MNIST with tf.Keras](https://medium.com/tensorflow/hello-deep-learning-fashion-mnist-with-keras-50fcff8cd74a)

## Resources

- [Deep Learning in Python](https://www.datacamp.com/courses/deep-learning-in-python) - DataCamp course (finished last week)
- [Convolution Neural Networks for Image Processing](https://www.datacamp.com/courses/convolutional-neural-networks-for-image-processing) - DataCamp course (started and completed last week)
- [Deep Learning with Python](https://www.manning.com/books/deep-learning-with-python) by François Chollet - Excellent resource by the creator of keras
- [TensorBoard](https://www.tensorflow.org/guide/summaries_and_tensorboard) - Fantastic visualization of neural network metrics

Thoughts? Comments? Please feel free to assist me by adding your voice to this project process.
